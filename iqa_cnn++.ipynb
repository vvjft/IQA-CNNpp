{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYZL4SczQfGeUSDAseDYS8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvjft/IQA-CNNpp/blob/main/iqa_cnn%2B%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install unrar\n",
        "#import logging # can be set once for session\n",
        "#logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%H:%M')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TKiIR9kOIS_",
        "outputId": "28781e0e-72db-4136-e836-115ab02d15fb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unrar is already the newest version (1:6.1.5-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "0-A0BbLrTPdM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import subprocess\n",
        "import os\n",
        "from scipy.signal import convolve2d\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import sys\n",
        "\n",
        "class database_loader:\n",
        "    ''' Parent class for database-specific loaders '''\n",
        "    def __init__(self):\n",
        "      self.catalogue = 'databases'\n",
        "\n",
        "      ### Attributes to be declared within the child class ###\n",
        "      self.url = ''      # URL of the dataset\n",
        "      self.exdir = ''      # Directory where the exctracted dataset is stored\n",
        "      self.score = ''    # MOS/DMOS column name\n",
        "      self.images = ''   # Directory where the images are stored\n",
        "      self.archive_file = '' # Path to the rar file\n",
        "      self.zip_file = '' # Path to the zip file\n",
        "\n",
        "    def data_exist(self):\n",
        "        '''Check if patch files are present in the directory.'''\n",
        "        return (os.path.exists(os.path.join(self.exdir, 'X_train.npy')) and os.path.exists(os.path.join(self.exdir, 'y_train.npy')) and\n",
        "                os.path.exists(os.path.join(self.exdir, 'X_val.npy')) and os.path.exists(os.path.join(self.exdir, 'y_val.npy')) and\n",
        "                os.path.exists(os.path.join(self.exdir, 'X_test.npy')) and os.path.exists(os.path.join(self.exdir, 'y_test.npy')))\n",
        "\n",
        "    def save_data(self, data, set_type):\n",
        "        '''Save the data to disk.'''\n",
        "        X, y = data\n",
        "        np.save(os.path.join(self.exdir, f'X_{set_type}.npy'), X)\n",
        "        np.save(os.path.join(self.exdir, f'y_{set_type}.npy'), y)\n",
        "        print(f\"{set_type} data saved successfully.\")\n",
        "\n",
        "    def load_data(self, set_type):\n",
        "        '''Load the data from disk.'''\n",
        "        X = np.load(os.path.join(self.exdir, f'X_{set_type}.npy'))\n",
        "        y = np.load(os.path.join(self.exdir, f'y_{set_type}.npy'))\n",
        "        return X, y\n",
        "\n",
        "    def download(self, extract_in='databases'):\n",
        "        '''Download the dataset from the URL and extract it to the directory.\n",
        "        Args:\n",
        "            file: (str) either rar_file or zip_file\n",
        "            extract_in (str, optional): Provide if the dataset is not extracted into a folder named after the file\n",
        "        Note:\n",
        "            You need to provide path into WinRAR or 7zip exe file.\n",
        "        '''\n",
        "        try:\n",
        "            if os.path.exists(self.images or self.data_exist):\n",
        "                print(\"Dataset already downloaded.\")\n",
        "                return True\n",
        "            if os.path.exists(self.archive_file):\n",
        "                print(f\"Extracting dataset from {self.archive_file}...\")\n",
        "            else:\n",
        "                print(f\"Downloading dataset from {self.url}...\")\n",
        "                !wget {self.url} -P {self.catalogue}\n",
        "            if self.archive_file.endswith('.rar'):\n",
        "                !unrar x -inul {file} {extract_in}\n",
        "            elif self.archive_file.endswith('.zip'):\n",
        "                !unzip -q {self.archive_file} -d {extract_in}\n",
        "            else:\n",
        "                print(f\"Unsupported file type: {self.archive_file}\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download or extract dataset: {e}.\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def split_data(self, data1):\n",
        "        train_data, test_data = train_test_split(data1, test_size=0.2, random_state=40)\n",
        "        train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=40)\n",
        "        return train_data, val_data, test_data\n",
        "\n",
        "    def preprocess(self, train_data, val_data, test_data, patch_size=32):\n",
        "\n",
        "        def normalize_image(patch, P=3, Q=3, C=1):\n",
        "            kernel = np.ones((P, Q)) / (P * Q)\n",
        "            patch_mean = convolve2d(patch, kernel, boundary='symm', mode='same')\n",
        "            patch_sm = convolve2d(np.square(patch), kernel, boundary='symm', mode='same')\n",
        "            patch_std = np.sqrt(np.maximum(patch_sm - np.square(patch_mean), 0)) + C\n",
        "            patch_ln = (patch - patch_mean) / patch_std\n",
        "            return patch_ln.astype('float32')\n",
        "\n",
        "        def slice_image(image, patch_size=32):\n",
        "            height, width = image.shape[:2]\n",
        "            num_patches_y = height // patch_size\n",
        "            num_patches_x = width // patch_size\n",
        "            patch_count = 0\n",
        "            for i in range(num_patches_y):\n",
        "                for j in range(num_patches_x):\n",
        "                    patch = image[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n",
        "                    patch_path = os.path.join(output_dir_patches, f\"{os.path.splitext(filename)[0]}_patch_{patch_count}.bmp\")\n",
        "                    patch_filename = f\"{os.path.splitext(filename)[0]}_patch_{patch_count}.bmp\"\n",
        "                    cv2.imwrite(patch_path, patch)\n",
        "                    self.patches.append([patch_filename, mos, distortion])\n",
        "                    patch_count += 1\n",
        "\n",
        "        sets = [(train_data, 'training'), (val_data, 'validation'), (test_data, 'test')]\n",
        "        dfs = []\n",
        "        total_images = sum(len(data) for data, _ in sets)\n",
        "        processed_images = 0\n",
        "        print('Preprocessing images...')\n",
        "\n",
        "        for (data, name) in sets:\n",
        "            output_dir_full = os.path.join(self.exdir, 'normalized_distorted_images', name, 'full')\n",
        "            output_dir_patches = os.path.join(self.exdir, 'normalized_distorted_images', name, 'patches')\n",
        "            os.makedirs(output_dir_full, exist_ok=True)\n",
        "            os.makedirs(output_dir_patches, exist_ok=True)\n",
        "            self.patches = []\n",
        "            for row in data.itertuples(index=False):\n",
        "                filename = row[0]\n",
        "                mos = row[1]\n",
        "                distortion = row[2]\n",
        "                image_path = os.path.join(self.images, filename)\n",
        "                image = cv2.imread(image_path)\n",
        "                if image is None:\n",
        "                    print(f\"Failed to load image: {filename}\")\n",
        "                    continue\n",
        "                image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "                image_normalized = normalize_image(image_gray)\n",
        "                filename = f'NORM_{filename.lower()}'\n",
        "                cv2.imwrite(os.path.join(output_dir_full, filename), image_normalized)\n",
        "                slice_image(image_normalized, patch_size)\n",
        "                processed_images += 1\n",
        "                sys.stdout.write(f'\\rProcessed {processed_images}/{total_images} images. ')\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            patches = pd.DataFrame(self.patches, columns=['image', self.score, 'distortion'])\n",
        "            dfs.append(patches)\n",
        "        return dfs\n",
        "\n",
        "    def encode(self, dataframes):\n",
        "        '''Encodes distortion labels into one-hot vectors.'''\n",
        "        for i in range(len(dataframes)):\n",
        "            dists = dataframes[i]['distortion']\n",
        "            le = LabelEncoder()\n",
        "            y_class_encoded = le.fit_transform(dists)\n",
        "            dists_one_hot = to_categorical(y_class_encoded, num_classes=13).astype(int)\n",
        "            dataframes[i]['distortion_encoded'] = [np.array(one_hot) for one_hot in dists_one_hot]\n",
        "            dataframes[i] = dataframes[i].drop(['distortion'], axis=1)\n",
        "        return dataframes\n",
        "\n",
        "    def map2tf(self, set_type, data):\n",
        "        '''\n",
        "        Maps data into format excected by TensorFlow: adds chanel dimension and stores data in arrays.\n",
        "        set_type: 'training', 'validation', 'test'\n",
        "        data: DataFrame with columns: 'image', 'MOS', 'distortion'\n",
        "        '''\n",
        "        images_dir = os.path.join(self.exdir, 'normalized_distorted_images', set_type, 'patches')\n",
        "        X, y = [], []\n",
        "        for row in data.itertuples(index=False):\n",
        "            filename = row[0]\n",
        "            score = row[1]\n",
        "            file_path = os.path.join(images_dir, filename)\n",
        "            if filename.endswith(('.bmp', '.png')) and os.path.exists(file_path):\n",
        "                img = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)\n",
        "                X.append(img)\n",
        "                y.append(score)\n",
        "            else:\n",
        "                print(f\"File not found: {file_path}\")\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        X = X[..., np.newaxis]\n",
        "        return X, y\n",
        "\n",
        "class tid2013_loader(database_loader):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.url = 'https://www.ponomarenko.info/tid2013/tid2013.rar'\n",
        "        self.exdir = os.path.join(self.catalogue, 'tid2013')\n",
        "        self.score = 'MOS'\n",
        "        self.images = os.path.join(self.exdir, 'distorted_images')\n",
        "        self.archive_file = os.path.join(self.catalogue, 'tid2013.rar')\n",
        "        self.distortion_mapping = {1: 'wn', 2:'wnc', 3:'scn', 4:'mn', 5:'hfn', 6:'in', 7:'qn', 8: 'gblur', 9:'idn', 10: 'jpeg', 11: 'jp2k', 12:'jpegte', 13:'jp2kte'} # According to TID2013 documentation\n",
        "\n",
        "        os.makedirs(self.exdir, exist_ok=True)\n",
        "\n",
        "        self.download(extract_in=self.exdir)\n",
        "\n",
        "        if self.data_exist():\n",
        "            print(\"Patch files found. Loading patched data...\")\n",
        "            self.train = self.load_data('train')\n",
        "            self.val = self.load_data('val')\n",
        "            self.test= self.load_data('test')\n",
        "        else:\n",
        "            data = self.prepare_data()\n",
        "            print('Mapping data to TensorFlow format...')\n",
        "            self.train = self.map2tf('training', data[0])\n",
        "            self.val = self.map2tf('validation', data[1])\n",
        "            self.test = self.map2tf('test', data[2])\n",
        "            self.save_data(self.train, 'train')\n",
        "            self.save_data(self.val, 'val')\n",
        "            self.save_data(self.test, 'test')\n",
        "        #self.train, self.val, self.test = self.encode(data)\n",
        "\n",
        "        print(\"Data loaded successfully.\")\n",
        "\n",
        "    def prepare_data(self, filter=True):\n",
        "        data_path = os.path.join(self.exdir, 'mos_with_names.txt')\n",
        "        data = pd.read_csv(data_path, header=None, delimiter=' ')\n",
        "        data = data.iloc[:, [1, 0]]  # swap column order\n",
        "        data.columns = ['image', 'MOS']\n",
        "        data['distortion'] = data['image'].apply(lambda x: self.distortion_mapping.get(int(x.split('_')[1]), 'other'))\n",
        "        if filter:\n",
        "            data = data[data['distortion'].isin(self.distortion_mapping.values())]\n",
        "        data.to_csv(os.path.join(self.exdir,'mos_with_names.csv'), index=False)\n",
        "\n",
        "        train_data, val_data, test_data = self.split_data(data)\n",
        "        train_data, val_data, test_data = self.preprocess(train_data, val_data, test_data)\n",
        "\n",
        "        return [train_data, val_data, test_data]\n",
        "\n",
        "class kadid10k_loader(database_loader):\n",
        "    def __init__(self, download=True):\n",
        "        super().__init__()\n",
        "        self.url = 'https://datasets.vqa.mmsp-kn.de/archives/kadid10k.zip'\n",
        "        self.exdir = os.path.join(self.catalogue, 'kadid10k')\n",
        "        self.score = 'DMOS'\n",
        "        self.images = os.path.join(self.exdir, 'images')\n",
        "        self.archive_file = os.path.join(self.catalogue, 'kadid10k.zip')\n",
        "        self.distortion_mapping = {1: 'gblur', 2: 'lblur', 3: 'mblur', 4: 'cdiff', 5: 'cshift', # According to KADID-10k documentation\n",
        "                                   6: 'cquant', 7: 'csat1', 8: 'csat2', 9: 'jp2k', 10: 'jpeg',\n",
        "                                   11: 'wniose1', 12: 'wniose2', 13: 'inoise', 14: 'mnoise', 15: 'denoise',\n",
        "                                   16: 'bright', 17: 'dark', 18: 'meanshft', 19: 'jit', 20: 'patch',\n",
        "                                   21: 'pixel', 22: 'quant', 23: 'cblock', 24: 'sharp', 25: 'contrst'}\n",
        "        self.download()\n",
        "\n",
        "        if self.data_exist():\n",
        "            print(\"Patch files found. Loading patched data...\")\n",
        "            self.train = self.load_data('train')\n",
        "            self.val = self.load_data('val')\n",
        "            self.test= self.load_data('test')\n",
        "        else:\n",
        "            data = self.prepare_data()\n",
        "            print('Mapping data to TensorFlow format...')\n",
        "            self.train = self.map2tf('training', data[0])\n",
        "            self.val = self.map2tf('validation', data[1])\n",
        "            self.test = self.map2tf('test', data[2])\n",
        "            self.save_data(self.train, 'train')\n",
        "            self.save_data(self.val, 'val')\n",
        "            self.save_data(self.test, 'test')\n",
        "        #self.train, self.val, self.test = self.encode(data)\n",
        "\n",
        "        print(\"Data loaded successfully.\")\n",
        "\n",
        "    def prepare_data(self, filter=True):\n",
        "        data_path = os.path.join(self.exdir, 'dmos.csv')\n",
        "        data = pd.read_csv(data_path, header=0, usecols=[0, 2])\n",
        "        data.columns = ['image', 'DMOS']\n",
        "        data['distortion'] = data['image'].apply(lambda x: self.distortion_mapping.get(int(x.split('_')[1]), 'other'))\n",
        "        #if filter:\n",
        "            #data = data[data['distortion'].isin(self.distortion_mapping.values())]\n",
        "        data.to_csv(os.path.join(self.exdir,'dmos_with_names.csv'), index=False)\n",
        "\n",
        "        train_data, val_data, test_data = self.split_data(data)\n",
        "        train_data, val_data, test_data = self.preprocess(train_data, val_data, test_data)\n",
        "\n",
        "        return [train_data, val_data, test_data]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the download is slow, stop it, delete rar/zip file (!rm)\n",
        " and run it again."
      ],
      "metadata": {
        "id": "k1nG-AgVZX5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r 'databases/tid2013'\n",
        "#!rm -r 'databases/kadid10k'\n",
        "!rm 'databases/tid2013.rar'\n",
        "#!rm 'databases/kadid10k.zip\n",
        "\n",
        "data_loader = tid2013_loader()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GLRKt2kneZP",
        "outputId": "7c67d953-f689-4b3f-ae10-f8c35a27da21"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset from https://www.ponomarenko.info/tid2013/tid2013.rar...\n",
            "--2024-08-09 15:59:21--  https://www.ponomarenko.info/tid2013/tid2013.rar\n",
            "Resolving www.ponomarenko.info (www.ponomarenko.info)... 191.101.104.11, 2a02:4780:1d:ead7:e7e6:a7e5:2a6c:87a\n",
            "Connecting to www.ponomarenko.info (www.ponomarenko.info)|191.101.104.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 957680241 (913M) [application/x-rar-compressed]\n",
            "Saving to: ‘databases/tid2013.rar’\n",
            "\n",
            "tid2013.rar         100%[===================>] 913.31M  55.2MB/s    in 1m 44s  \n",
            "\n",
            "2024-08-09 16:01:07 (8.75 MB/s) - ‘databases/tid2013.rar’ saved [957680241/957680241]\n",
            "\n",
            "Preprocessing images...\n",
            "Processed 1625/1625 images. Mapping data to TensorFlow format...\n",
            "train data saved successfully.\n",
            "val data saved successfully.\n",
            "test data saved successfully.\n",
            "Data loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = data_loader.train\n",
        "X_val, y_val = data_loader.val\n",
        "X_test, y_test = data_loader.test"
      ],
      "metadata": {
        "id": "S_E9JdNaIrI9"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(32, 32, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZgZvm-Oaw5x",
        "outputId": "f8f13b20-80f8-47b5-e197-7ba5458e6d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5850/5850\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 13ms/step - loss: 2.7539 - mae: 1.1926 - val_loss: 1.2005 - val_mae: 0.8918\n",
            "Epoch 2/10\n",
            "\u001b[1m5850/5850\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 12ms/step - loss: 1.0927 - mae: 0.8435 - val_loss: 1.1616 - val_mae: 0.8660\n",
            "Epoch 3/10\n",
            "\u001b[1m5850/5850\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 12ms/step - loss: 1.0388 - mae: 0.8167 - val_loss: 1.0642 - val_mae: 0.8357\n",
            "Epoch 4/10\n",
            "\u001b[1m5850/5850\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 12ms/step - loss: 0.9963 - mae: 0.7955 - val_loss: 1.0608 - val_mae: 0.8234\n",
            "Epoch 5/10\n",
            "\u001b[1m5850/5850\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 12ms/step - loss: 0.9552 - mae: 0.7760 - val_loss: 1.0541 - val_mae: 0.8222\n",
            "Epoch 6/10\n",
            "\u001b[1m5850/5850\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 12ms/step - loss: 0.9264 - mae: 0.7633 - val_loss: 1.0432 - val_mae: 0.8091\n",
            "Epoch 7/10\n",
            "\u001b[1m5850/5850\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 12ms/step - loss: 0.9018 - mae: 0.7516 - val_loss: 1.0211 - val_mae: 0.8011\n",
            "Epoch 8/10\n",
            "\u001b[1m5850/5850\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 13ms/step - loss: 0.8772 - mae: 0.7401 - val_loss: 1.0261 - val_mae: 0.7998\n",
            "Epoch 9/10\n",
            "\u001b[1m 142/5850\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 14ms/step - loss: 0.8621 - mae: 0.7345"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', loss)\n",
        "print('Test MAE:', mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1YpQZcgZAgF",
        "outputId": "21d4ad66-2647-4d9c-c45b-efa79baf0a57"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.047837734222412\n",
            "Test MAE: 0.8069906830787659\n"
          ]
        }
      ]
    }
  ]
}